{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ITvyYx54g5C"
   },
   "source": [
    "## Classification with sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1J4daS74g5D"
   },
   "source": [
    "## loading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u-h1T5rE4g5E"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_file(url,local_file, force=False):\n",
    "    \"\"\"\n",
    "    Helper function to download a file and store it locally\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_file) or force:\n",
    "        print('Downloading',url,'to',local_file)\n",
    "        with urllib.request.urlopen(url) as opener, \\\n",
    "             open(local_file, mode='w', encoding='utf-8') as outfile:\n",
    "                    outfile.write(opener.read().decode('utf-8'))\n",
    "    else:\n",
    "        print(local_file,'already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uO0fkbdn4g5J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.esuli.it/demo/data/news_en_train.csv to news_en_train.txt\n",
      "Downloading http://www.esuli.it/demo/data/news_en_test.csv to news_en_test.txt\n"
     ]
    }
   ],
   "source": [
    "train_file = 'news_en_train.txt'\n",
    "train_url='http://www.esuli.it/demo/data/news_en_train.csv'\n",
    "test_file = 'news_en_test.txt'\n",
    "test_url = 'http://www.esuli.it/demo/data/news_en_test.csv'\n",
    "delimiter = ','\n",
    "\n",
    "download_file(train_url, train_file)\n",
    "download_file(test_url, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3zwOQQ64g5O"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "x_train = list()\n",
    "y_train = list()\n",
    "with open(train_file, encoding='utf-8', newline='') as infile:\n",
    "    reader = csv.reader(infile, delimiter=delimiter)\n",
    "    for row in reader:\n",
    "        x_train.append(row[0])\n",
    "        y_train.append(row[1])\n",
    "\n",
    "x_test = list()\n",
    "y_test = list()\n",
    "with open(test_file, encoding='utf-8', newline='') as infile:\n",
    "    reader = csv.reader(infile, delimiter=delimiter)\n",
    "    for row in reader:\n",
    "        x_test.append(row[0])\n",
    "        y_test.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70X8gxqX4g5T"
   },
   "outputs": [],
   "source": [
    "len(x_train),len(y_train),len(x_test),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62X87ZY84g5Z"
   },
   "outputs": [],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r42x4ICg4g5e"
   },
   "outputs": [],
   "source": [
    "sample_idx = 10\n",
    "x_train[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1olVl9d54g5j"
   },
   "outputs": [],
   "source": [
    "y_train[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Sw_15ILDQ8F"
   },
   "source": [
    "# Binary classification\n",
    "\n",
    "This is a multi-class single-label dataset.\n",
    "We start with a simpler binary classification problem, e.g., economy vs not economy.\n",
    "\n",
    "Just to make a choice, we use as the reference label the one of the example in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQtlnmjUDmvm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy implements many useful and powerful vector manipulation tools\n",
    "# here I'm using it to quickly create a True,False vector corresponding\n",
    "# to the original values being equal to our label of interest or not\n",
    "# i.e., binary labels\n",
    "\n",
    "y_train_bin = np.asarray(y_train)==y_train[sample_idx]\n",
    "y_test_bin = np.asarray(y_test)==y_train[sample_idx]\n",
    "y_train_bin,y_test_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwMzU9JR4g5o"
   },
   "source": [
    "## Building the pipeline by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c26tzsE4g5p"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VWoWZd_7uIe"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "Try the following two cells removing the min_df parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vk5Cvvyc4g5r"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5)  # tokenization and frequency count\n",
    "\n",
    "print('fit')\n",
    "vect.fit(x_train)\n",
    "print('transform')\n",
    "X_train_tok = vect.transform(x_train)\n",
    "print('done')\n",
    "\n",
    "# the two steps above can be condensed in a single step that processes train\n",
    "# data only once.\n",
    "\n",
    "# print('fit_transform')\n",
    "# X_train_tok = vect.fit_transform(x_train)\n",
    "# print('done')\n",
    "\n",
    "X_test_tok =vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1x3p0T271jX"
   },
   "outputs": [],
   "source": [
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxwBNFrT8UQ0"
   },
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipJykPr14g5u"
   },
   "outputs": [],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2lgD7Y04g5x"
   },
   "outputs": [],
   "source": [
    "X_train_tok[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mik5K904g51"
   },
   "outputs": [],
   "source": [
    "print(X_train_tok[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nopxC-YP4g54"
   },
   "source": [
    "Some scikit-learn modules implement an inverse_transform method to reconstruct input from their output.\n",
    "Let's print out the feature names and their frequency for a document. Note that frequency info is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjh8ymz_4g55"
   },
   "outputs": [],
   "source": [
    "vect.inverse_transform(X_train_tok[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FziDW-YRBGRP"
   },
   "source": [
    "Let's attach frequency data to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gU1QgQx88iR"
   },
   "outputs": [],
   "source": [
    "for feat,freq in zip(vect.inverse_transform(X_train_tok[0,:])[0],X_train_tok[0,:].data):\n",
    "  print(feat,freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHFnwDZO7Ucs"
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "This is the first element where we use the labels, because it is a supervised method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_G6-jLSx4g5-"
   },
   "outputs": [],
   "source": [
    "bin_sel = SelectKBest(chi2, k=5000)  # feature selection\n",
    "bin_sel.fit(X_train_tok,y_train_bin)\n",
    "X_train_sel_bin = bin_sel.transform(X_train_tok)\n",
    "X_test_sel_bin = bin_sel.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_tPAeRm4g6C"
   },
   "outputs": [],
   "source": [
    "bin_sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_PneJ3w4g6G"
   },
   "outputs": [],
   "source": [
    "X_train_sel_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcZ-oyJYBlTI"
   },
   "outputs": [],
   "source": [
    "X_train_sel_bin[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVS4Y3cr4g6L"
   },
   "outputs": [],
   "source": [
    "print(X_train_sel_bin[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Za9mLr17DRm"
   },
   "source": [
    "The feature selection module has an inverse transform method so that we can map selected feature back to the original large feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBK9OqB0673F"
   },
   "outputs": [],
   "source": [
    "bin_sel.inverse_transform(X_train_sel_bin[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDR_BxTG7Ov-"
   },
   "outputs": [],
   "source": [
    "print(vect.inverse_transform(bin_sel.inverse_transform(X_train_sel_bin[0,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jqGqAmC7P1c"
   },
   "source": [
    "## Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdGwiuCW4g6P"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()  # weighting\n",
    "tfidf.fit(X_train_sel_bin)\n",
    "X_train_vec_bin = tfidf.transform(X_train_sel_bin)\n",
    "X_test_vec_bin =tfidf.transform(X_test_sel_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL7RI16F4g6R"
   },
   "outputs": [],
   "source": [
    "print(X_train_vec_bin[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQhAOByVBtId"
   },
   "outputs": [],
   "source": [
    "for feat,weight,freq in zip(vect.inverse_transform(bin_sel.inverse_transform(X_train_vec_bin[0,:]))[0],X_train_vec_bin[0,:].data,X_train_sel_bin[0,:].data):\n",
    "  print(feat,weight,freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnoJ4nKpCjKI"
   },
   "source": [
    "## Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_okA1Ms4g6T"
   },
   "outputs": [],
   "source": [
    "svm_bin = LinearSVC()  # linear svm with default parameters\n",
    "svm_bin_clf = svm_bin.fit(X_train_vec_bin,y_train_bin)\n",
    "bin_predictions = svm_bin_clf.predict(X_test_vec_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhprQb044g6X"
   },
   "outputs": [],
   "source": [
    "len(bin_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txJRiT3y4g6Z"
   },
   "outputs": [],
   "source": [
    "bin_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O3PFCjz4g6b"
   },
   "source": [
    "## Evaluation of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-xBzyB24g6b"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for prediction,true_label in zip(bin_predictions, y_test_bin):\n",
    "    if prediction==true_label:\n",
    "        correct += 1\n",
    "print(correct/len(bin_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_heVYmB4g6e"
   },
   "source": [
    "## Using sklearn pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXKht0yn4g6f"
   },
   "outputs": [],
   "source": [
    "bin_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2, k=5000)),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', LinearSVC())  # learning algorithm\n",
    "])\n",
    "\n",
    "bin_pipeline.fit(x_train,y_train_bin)\n",
    "bin_predictions = bin_pipeline.predict(x_test)\n",
    "correct = 0\n",
    "for prediction,true_label in zip(bin_predictions, y_test_bin):\n",
    "    if prediction==true_label:\n",
    "        correct += 1\n",
    "print(correct/len(bin_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJYsmcs-4g6h"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test_bin, bin_predictions))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(y_test_bin, bin_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5tBkYkQKCx5"
   },
   "source": [
    "## Inspecting the pipeline\n",
    "\n",
    "We can have a look at the parameters of the supervised method of the pipeline to understand how it determines its classification decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCXw4sBWKGmq"
   },
   "outputs": [],
   "source": [
    "tokenizer = bin_pipeline.named_steps['vect']\n",
    "selector = bin_pipeline.named_steps['sel']\n",
    "classifier = bin_pipeline.named_steps['learner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b7_u7ZdMA40"
   },
   "source": [
    "First we look at the feature selection function.\n",
    "We get the chi^2 score assigned to every feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPkxt22uKFp3"
   },
   "outputs": [],
   "source": [
    "feature_names = tokenizer.get_feature_names()\n",
    "feats_w_score = list()\n",
    "for index,(selected,score) in enumerate(zip(selector.get_support(),selector.scores_)):\n",
    "    feats_w_score.append((score,selected,feature_names[index]))\n",
    "feats_w_score = sorted(feats_w_score)\n",
    "len(feats_w_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R8YA5UQMJCN"
   },
   "source": [
    "This are the 100 less and most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRfOn0prLXEN"
   },
   "outputs": [],
   "source": [
    "feats_w_score[:100],feats_w_score[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvMEnIUIMPOS"
   },
   "source": [
    "Then we look at the parameters of the linear classification model.\n",
    "Values with highest absolute values are those which contribute the most to the classification decision. Values close to zero are less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Vh7f5gjLXbq"
   },
   "outputs": [],
   "source": [
    "feats_w_classifier_weight = list()\n",
    "for index,weight in enumerate(selector.inverse_transform(classifier.coef_)[0]):\n",
    "    if weight!=0:\n",
    "        feats_w_classifier_weight.append((weight,feature_names[index]))\n",
    "feats_w_classifier_weight = sorted(feats_w_classifier_weight)\n",
    "len(feats_w_classifier_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGTRTJUBNV1U"
   },
   "source": [
    "These are the feature that most contribute to a positive decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLTtt2XfLW0X"
   },
   "outputs": [],
   "source": [
    "feats_w_classifier_weight[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma0lTZK6NbTu"
   },
   "source": [
    "These are the features that most contribute to a negative decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dT9TNnfLNeSO"
   },
   "outputs": [],
   "source": [
    "feats_w_classifier_weight[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ8qwGxGKlFG"
   },
   "source": [
    "## Testing other classifiers\n",
    "\n",
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSUxX-ykKqJM"
   },
   "outputs": [],
   "source": [
    "dt_bin_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2, k=5000)),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', DecisionTreeClassifier())  # learning algorithm\n",
    "])\n",
    "\n",
    "dt_bin_pipeline.fit(x_train,y_train_bin)\n",
    "bin_predictions = dt_bin_pipeline.predict(x_test)\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test_bin, bin_predictions))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(y_test_bin, bin_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-QPQUXSRIf0"
   },
   "source": [
    "We can try to visualize the tree, but there are too many dimension to have a structure that is really inspectable (I'm referring to the font size, but to the number of nodes of the tree!).\n",
    "\n",
    "DT visualization works on low dimensional data (see https://scikit-learn.org/stable/modules/tree.html#classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhxd-9zGOL7M"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(24, 24))\n",
    "plot_tree(dt_bin_pipeline.named_steps['learner'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-A-GRT2Rpx6"
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "NB uses a multinomial model based on term frequencies, we can skip the tfidf module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z07gZuFtRrHg"
   },
   "outputs": [],
   "source": [
    "nb_bin_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2, k=5000)),  # feature selection\n",
    "    ('learner', MultinomialNB())  # learning algorithm\n",
    "])\n",
    "\n",
    "nb_bin_pipeline.fit(x_train,y_train_bin)\n",
    "bin_predictions = nb_bin_pipeline.predict(x_test)\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test_bin, bin_predictions))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(y_test_bin, bin_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2P0xp4KRqYp"
   },
   "outputs": [],
   "source": [
    "tokenizer = nb_bin_pipeline.named_steps['vect']\n",
    "selector = nb_bin_pipeline.named_steps['sel']\n",
    "classifier = nb_bin_pipeline.named_steps['learner']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vza7RhZwUruQ"
   },
   "source": [
    "NB model stores log values of priors and likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWviX39KUpfS"
   },
   "outputs": [],
   "source": [
    "classifier.class_log_prior_,classifier.feature_log_prob_, len(classifier.feature_log_prob_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NliswasKUpJf"
   },
   "source": [
    "In NB a key factor for decision is the ratio between the likelihood for positive and negative decision.\n",
    "\n",
    "The next cell exploits numpy to perform element-by-element division between log probabilities of p(w|class=1) and p(w|class=0), producing a vector of such ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnNFViKlWd2Z"
   },
   "outputs": [],
   "source": [
    "ratio = classifier.feature_log_prob_[0]/classifier.feature_log_prob_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl26kPT-VcZR"
   },
   "outputs": [],
   "source": [
    "feats_w_classifier_weight = list()\n",
    "feature_names = tokenizer.get_feature_names()\n",
    "for index,weight in enumerate(selector.inverse_transform([ratio])[0]):\n",
    "    if weight!=0:\n",
    "        feats_w_classifier_weight.append((weight,feature_names[index]))\n",
    "feats_w_classifier_weight = sorted(feats_w_classifier_weight)\n",
    "len(feats_w_classifier_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeC1pC2kXjj2"
   },
   "source": [
    "This are the most relevant features for a positive decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ji_oDB8WGTt"
   },
   "outputs": [],
   "source": [
    "feats_w_classifier_weight[-100::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7rnjWY0XrYj"
   },
   "source": [
    "These are the most relevat features for a negative decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "520pEG9jXr31"
   },
   "outputs": [],
   "source": [
    "feats_w_classifier_weight[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KCHhR_bExfa"
   },
   "source": [
    "# Multi-class single-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUl_XQlGE53g"
   },
   "source": [
    "Tokenization does not change from the binary problem, as the dataset is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-vKvHMkE073"
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "Here we use single-label labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYXqOJy7E075"
   },
   "outputs": [],
   "source": [
    "sel = SelectKBest(chi2, k=5000)  # feature selection\n",
    "sel.fit(X_train_tok,y_train)\n",
    "X_train_sel = sel.transform(X_train_tok)\n",
    "X_test_sel = sel.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uZCJLrGE08G"
   },
   "outputs": [],
   "source": [
    "sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLT0SCitE08L"
   },
   "outputs": [],
   "source": [
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyK7_GdjE08W"
   },
   "outputs": [],
   "source": [
    "X_train_sel[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJIlACkxE08b"
   },
   "outputs": [],
   "source": [
    "print(X_train_sel[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1qbRIgeE08g"
   },
   "source": [
    "Selected feature differ from the binary case, as now they have to be informative with respect to a different set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVbxoMzcE08l"
   },
   "outputs": [],
   "source": [
    "print(vect.inverse_transform(sel.inverse_transform(X_train_sel[0,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqtNzTeME08o"
   },
   "source": [
    "## Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kddCsL0dE08o"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()  # weighting\n",
    "tfidf.fit(X_train_sel)\n",
    "X_train_vec = tfidf.transform(X_train_sel)\n",
    "X_test_vec =tfidf.transform(X_test_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTQeGJJrE08q"
   },
   "outputs": [],
   "source": [
    "print(X_train_vec[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-7IAy_2E08s"
   },
   "outputs": [],
   "source": [
    "for feat,weight in zip(vect.inverse_transform(sel.inverse_transform(X_train_vec[0,:]))[0],X_train_vec[0,:].data):\n",
    "  print(feat,weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUEjfuYlE08u"
   },
   "source": [
    "## Learning algorithm\n",
    "\n",
    "Linear SVM implement multi-class single-label using a one-vs-rest approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyIr1ypPE08u"
   },
   "outputs": [],
   "source": [
    "learner = LinearSVC()  # linear svm with default parameters\n",
    "classifier = learner.fit(X_train_vec,y_train)\n",
    "predictions = classifier.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_zL5N2-E08w"
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqnqooVyE080"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxllwx8E083"
   },
   "source": [
    "## Evaluation of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAJWosMxE084"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for prediction,true_label in zip(predictions, y_test):\n",
    "    if prediction==true_label:\n",
    "        correct += 1\n",
    "print(correct/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay23cQTUE087"
   },
   "source": [
    "## Using sklearn pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3y7s_LKE087"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2, k=5000)),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', LinearSVC())  # learning algorithm\n",
    "])\n",
    "\n",
    "classifier = pipeline.fit(x_train,y_train)\n",
    "predictions = classifier.predict(x_test)\n",
    "correct = 0\n",
    "for prediction,true_label in zip(predictions, y_test):\n",
    "    if prediction==true_label:\n",
    "        correct += 1\n",
    "print(correct/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-rQMii8E089"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD-76IqUZ7Vp"
   },
   "source": [
    "The classification score for the binary classifier we learned earlier is different, though it is trained on exactly the same data. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkxDjHG-aSyo"
   },
   "source": [
    "We try a linear svm with one-vs-one model.\n",
    "\n",
    "LinearSVC does not implement OvO.\n",
    "\n",
    "We can wrap it into a OneVsOneClassifier that can be applied to any classifier.\n",
    "\n",
    "(Note that other classifiers natively implement OvO, e.g., sklearn.svm.SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7OH-NU6E08_"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2, k=5000)),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', OneVsOneClassifier(LinearSVC()))  # learning algorithm\n",
    "])\n",
    "\n",
    "classifier = pipeline.fit(x_train,y_train)\n",
    "predictions = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewi5pJIraQPo"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuYEx5SEpcYo"
   },
   "source": [
    "# Saving classifiers\n",
    "\n",
    "Fitted classifiers (both single object and pipelines), as any scikit object, can be saved and the load for successive reuse.\n",
    "\n",
    "NOTE: saving a file on Colab saves it on the temporary virtual machine on the cloud, to get a persistent copy additional code is require see https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKq5z7stnzze"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmv27GDzqEji"
   },
   "outputs": [],
   "source": [
    "with open('news_en_classifier.pkl',mode='bw') as outputfile:\n",
    "  pickle.dump(pipeline,outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4uvKaSaqKvl"
   },
   "outputs": [],
   "source": [
    "with open('news_en_classifier.pkl',mode='br') as inputfile:\n",
    "  pipeline = pickle.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kz9Md3G6rI5q"
   },
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdONvx-4rOG2"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('news_en_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1C9OSs9dn4L"
   },
   "outputs": [],
   "source": [
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWW76u5EdzqX"
   },
   "outputs": [],
   "source": [
    "with open('news_en_classifier (1).pkl',mode='br') as inputfile:\n",
    "  pipeline2 = pickle.load(inputfile)\n",
    "pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kQl_2CpfYLK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "06.1 - Classification - sklearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
